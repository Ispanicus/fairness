{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness, Accountability, and Ethics, Spring 2023\n",
    "\n",
    "## Mandatory Assignment 1\n",
    "\n",
    "Please use the following code to prepare the dataset.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folktables\n",
    "# !pip install hvplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "data_source = ACSDataSource(survey_year=\"2018\", horizon=\"1-Year\", survey=\"person\")\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "feature_names = [\n",
    "    \"AGEP\",  # Age\n",
    "    \"CIT\",  # Citizenship status\n",
    "    \"COW\",  # Class of worker\n",
    "    \"ENG\",  # Ability to speak English\n",
    "    \"SCHL\",  # Educational attainment\n",
    "    \"MAR\",  # Marital status\n",
    "    \"HINS1\",  # Insurance through a current or former employer or union\n",
    "    \"HINS2\",  # Insurance purchased directly from an insurance company\n",
    "    \"HINS4\",  # Medicaid\n",
    "    \"RAC1P\",  # Recoded detailed race code\n",
    "    \"SEX\",\n",
    "]\n",
    "\n",
    "target_name = \"PINCP\"  # Total person's income\n",
    "\n",
    "\n",
    "def data_processing(data, features, target_name: str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df[\"AGEP\"] > 16]\n",
    "    df = df[df[\"PINCP\"] > 100]\n",
    "    df = df[df[\"WKHP\"] > 0]\n",
    "    df = df[df[\"PWGTP\"] >= 1]\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[\n",
    "        features + [\"target\", target_name]\n",
    "    ]  ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "\n",
    "    # For some columns, we have binary values of 1 and 2, meaning they either have it or dont\n",
    "    # There's no reason to dummy encode these, however:\n",
    "    # It's extremely unintuitive that a value of 2 means that someone doesn't have insurance, therefore we convert to traditional 0=false, 1=true\n",
    "    # We do this by bit magic: if we minus by 2, then 1 -> -1, but converting this to a bool and back to an int, we get 1 -> -1 -> True -> 1\n",
    "    # Then we correctly achieve 0=false and 1=true\n",
    "    cols_where_2_is_falses = [\n",
    "        \"HINS1\",\n",
    "        \"HINS2\",\n",
    "        \"HINS4\",\n",
    "    ]\n",
    "    df_processed.loc[:, cols_where_2_is_falses] = (\n",
    "        (df_processed[[\"HINS1\", \"HINS2\", \"HINS4\"]] - 2).astype(bool).astype(int)\n",
    "    )\n",
    "    cols = [\"CIT\", \"COW\", \"SCHL\", \"MAR\", \"SEX\", \"RAC1P\"]\n",
    "    df_processed = pd.get_dummies(\n",
    "        df_processed,\n",
    "        prefix=None,\n",
    "        prefix_sep=\"_\",\n",
    "        dummy_na=False,\n",
    "        columns=cols,\n",
    "        drop_first=True,\n",
    "    )\n",
    "    df_processed = pd.get_dummies(\n",
    "        df_processed,\n",
    "        prefix=None,\n",
    "        prefix_sep=\"_\",\n",
    "        dummy_na=True,\n",
    "        columns=[\"ENG\"],\n",
    "        drop_first=True,\n",
    "    )\n",
    "    return df_processed, df, target, sex\n",
    "\n",
    "\n",
    "data, data_original, target, group = data_processing(\n",
    "    acs_data.sample(10_000, random_state=0), feature_names, target_name\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# from sklearn.svm import SVC # Too slow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "import hvplot.pandas\n",
    "import holoviews as hv\n",
    "import random\n",
    "import numpy\n",
    "hv.notebook_extension('bokeh')\n",
    "numpy.random.seed(0)\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = make_pipeline(MinMaxScaler(), LogisticRegression(random_state=0))\n",
    "clf2 = make_pipeline(\n",
    "    MinMaxScaler(), RandomForestClassifier(max_depth=3, random_state=0)\n",
    ")\n",
    "clfs = [clf1, clf2]\n",
    "for clf in clfs:\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "train_group_masks = [group_train == 1, group_train == 2]\n",
    "test_group_masks = [group_test == 1, group_test == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline accuracy\")\n",
    "for i, clf in enumerate(clfs):\n",
    "    Accuracy = round(clf.score(X_test, y_test), 3)\n",
    "    print(clf[1], \"\\n\", Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [\n",
    "    [clf.score(X_test[mask], y_test[mask]) for mask in test_group_masks] for clf in clfs\n",
    "]\n",
    "pd.DataFrame(\n",
    "    scores, columns=[\"Group 1\", \"Group 2\"], index=[\"WhiteBox\", \"BlackBox\"]\n",
    ").rename_axis(\"Baseline accuracy [%]\").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame([], index=[\"WhiteBox\", \"BlackBox\"])\n",
    "df_boxes = [\"WhiteBox\", \"BlackBox\"]\n",
    "\n",
    "positives = [\n",
    "    [clf.predict(X_test[mask]).mean() for mask in test_group_masks] for clf in clfs\n",
    "]\n",
    "df_metrics.loc[\"WhiteBox\", \"SP_1\"] = positives[0][0]\n",
    "df_metrics.loc[\"WhiteBox\", \"SP_2\"] = positives[0][1]\n",
    "df_metrics.loc[\"BlackBox\", \"SP_1\"] = positives[1][0]\n",
    "df_metrics.loc[\"BlackBox\", \"SP_2\"] = positives[1][1]\n",
    "parity_df = (\n",
    "    pd.DataFrame(\n",
    "        positives, columns=[\"Group 1\", \"Group 2\"], index=[\"WhiteBox\", \"BlackBox\"]\n",
    "    )\n",
    "    .rename_axis(\"Baseline positive [%]\")\n",
    "    .round(2)\n",
    ")\n",
    "parity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = (\n",
    "    parity_df.stack()\n",
    "    .rename_axis([\"box\", \"category\"])\n",
    "    .rename(\"prob\")\n",
    "    .reset_index()\n",
    "    .assign(color=[\"#4477AA\", \"#EE6677\"] * 2)\n",
    "    .replace({\"Group 1\": \"Pr(Selection | Male)\", \"Group 2\": \"Pr(Selection | Female)\"})\n",
    ")\n",
    "plots = plot_df.hvplot.bar(\n",
    "    x=\"category\", width=600, groupby=\"box\", color=\"color\", invert=True, ylim=(0, 1)\n",
    ")\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parity_plots = (\n",
    "    plots[\"WhiteBox\"].opts(title=\"Statistical Parity (Whitebox)\")\n",
    "    + plots[\"BlackBox\"].opts(\n",
    "        width=400, yaxis=None, title=\"Statistical Parity (Blackbox)\"\n",
    "    )\n",
    ").opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_df(group, group_no, clf):\n",
    "    prob = pd.Series(clf.predict_proba(X_train[group])[:, 1], name=\"prob\")\n",
    "    T = pd.Series(y_train[group], name=\"T\")\n",
    "    G = pd.Series([bool(group_no)] * len(T), name=\"G\")\n",
    "    return pd.concat([prob, G, T], axis=1)\n",
    "\n",
    "\n",
    "for i, box in enumerate(df_boxes):\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            get_group_df(group, group_no, clfs[i])\n",
    "            for group_no, group in enumerate(train_group_masks)\n",
    "        ],\n",
    "    )\n",
    "    df[\"S\"] = df[\"prob\"] * 100 > 50\n",
    "    # G is group\n",
    "    # T is target\n",
    "    # S is prediction/selection\n",
    "    N = lambda string: len(df.query(string))\n",
    "    df_metrics.loc[box, \"E_odd_t0_g0\"] = N(\"~T and S and ~G\") / N(\"~G and ~T\")\n",
    "    df_metrics.loc[box, \"E_odd_t0_g1\"] = N(\"~T and S and G\") / N(\"G and ~T\")\n",
    "    df_metrics.loc[box, \"E_odd_t1_g0\"] = N(\"T and S and ~G\") / N(\"~G and T\")\n",
    "    df_metrics.loc[box, \"E_odd_t1_g1\"] = N(\"T and S and G\") / N(\"G and T\")\n",
    "    # Equalized outcome 0 is precisio\n",
    "    df_metrics.loc[box, \"E_out_s0_g0\"] = N(\"~G and T and ~S\") / N(\"~G and ~S\")\n",
    "    df_metrics.loc[box, \"E_out_s0_g1\"] = N(\"G and T and ~S\") / N(\"G and ~S\")\n",
    "    df_metrics.loc[box, \"E_out_s1_g0\"] = N(\"~G and T and S\") / N(\"~G and S\")\n",
    "    df_metrics.loc[box, \"E_out_s1_g1\"] = N(\"G and T and S\") / N(\"G and S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    \"E_odd_t0_g0\": \"Pr(Selection | low income, Male)\",\n",
    "    \"E_odd_t0_g1\": \"Pr(Selection | low income, Female)\",\n",
    "    \"E_odd_t1_g0\": \"Pr(Selection | high income, Male)\",\n",
    "    \"E_odd_t1_g1\": \"Pr(Selection | high income, Female)\",\n",
    "    \"E_out_s0_g0\": \"Pr(High income | not selected, Male)\",\n",
    "    \"E_out_s0_g1\": \"Pr(High income | not selected, Female)\",\n",
    "    \"E_out_s1_g0\": \"Pr(High income | selected, Male)\",\n",
    "    \"E_out_s1_g1\": \"Pr(High income | selected, Female)\",\n",
    "}\n",
    "color_map = {\n",
    "    \"E_odd_t0_g0\": \"#4477AA\",\n",
    "    \"E_odd_t0_g1\": \"#EE6677\",\n",
    "    \"E_odd_t1_g0\": \"#4477AA\",\n",
    "    \"E_odd_t1_g1\": \"#EE6677\",\n",
    "    \"E_out_s0_g0\": \"#4477AA\",\n",
    "    \"E_out_s0_g1\": \"#EE6677\",\n",
    "    \"E_out_s1_g0\": \"#4477AA\",\n",
    "    \"E_out_s1_g1\": \"#EE6677\",\n",
    "}\n",
    "\n",
    "plot_kwargs = dict(\n",
    "    x=\"category\",\n",
    "    invert=True,\n",
    "    ylim=(0, 1),\n",
    "    color=\"color\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_eq_odds_plot(model_type: str, df_metrics=df_metrics):\n",
    "    df = (\n",
    "        df_metrics[[col for col in df_metrics if \"odd\" in col]]\n",
    "        .loc[model_type]\n",
    "        .reset_index()\n",
    "    )\n",
    "    plot_df = df.assign(\n",
    "        category=df[\"index\"].map(name_map),\n",
    "        color=df[\"index\"].map(color_map),\n",
    "    )\n",
    "    return plot_df.hvplot.bar(\n",
    "        title=f\"Equalized Odds ({model_type})\", y=model_type, **plot_kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def get_eq_out_plot(model_type: str, df_metrics=df_metrics):\n",
    "    df = (\n",
    "        df_metrics[[col for col in df_metrics if \"out\" in col]]\n",
    "        .loc[model_type]\n",
    "        .reset_index()\n",
    "    )\n",
    "    plot_df = df.assign(\n",
    "        category=df[\"index\"].map(name_map),\n",
    "        color=df[\"index\"].map(color_map),\n",
    "    )\n",
    "    return plot_df.hvplot.bar(\n",
    "        title=f\"Equalized Outcomes ({model_type})\", y=model_type, **plot_kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "plots = (\n",
    "    (\n",
    "        parity_plots[0].opts(width=450)\n",
    "        + parity_plots[1].opts(width=250)\n",
    "        + get_eq_odds_plot(\"WhiteBox\").opts(width=450)\n",
    "        + get_eq_odds_plot(\"BlackBox\").opts(yaxis=None, width=250)\n",
    "        + get_eq_out_plot(\"WhiteBox\").opts(width=450)\n",
    "        + get_eq_out_plot(\"BlackBox\").opts(yaxis=None, width=250)\n",
    "    )\n",
    "    .cols(2)\n",
    "    .opts(shared_axes=False)\n",
    "    .opts(hv.opts.Bars(ylabel=\"\", xlabel=\"\", xaxis=None, height=150))\n",
    ")\n",
    "plots[-1].opts(xaxis=True)\n",
    "plots[-2].opts(xaxis=True)\n",
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(clf):\n",
    "    def get_group_df(group, group_no):\n",
    "        prob = pd.Series(clf.predict_proba(X_train[group])[:, 1], name=\"prob\")\n",
    "        T = pd.Series(y_train[group], name=\"T\")\n",
    "        G = pd.Series([bool(group_no)] * len(T), name=\"G\")\n",
    "        return pd.concat([prob, G, T], axis=1)\n",
    "\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            get_group_df(group, group_no)\n",
    "            for group_no, group in enumerate(train_group_masks)\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    ROC_g = []\n",
    "    ROC_G = []\n",
    "    for threshold in range(5, 100, 5):\n",
    "        df[\"S\"] = df[\"prob\"] * 100 > threshold\n",
    "        # TP_G = Count(S, G, T) / Count(G, T)\n",
    "        # FP_G = Count(S, G, ~T) / Count(G, ~T)\n",
    "        TP_g = len(df.query(\"S and ~G and T\")) / len(df.query(\"~G and T\"))\n",
    "        FP_g = len(df.query(\"S and ~G and ~T\")) / len(df.query(\"~G and ~T\"))\n",
    "        ROC_g.append([TP_g, FP_g, threshold])\n",
    "        TP_G = len(df.query(\"S and G and T\")) / len(df.query(\"G and T\"))\n",
    "        FP_G = len(df.query(\"S and G and ~T\")) / len(df.query(\"G and ~T\"))\n",
    "        ROC_G.append([TP_G, FP_G, threshold])\n",
    "\n",
    "        if threshold == 50:\n",
    "            g_mid = [[TP_g, FP_g, 50]]\n",
    "            G_mid = [[TP_G, FP_G, 50]]\n",
    "\n",
    "    kwargs = dict(\n",
    "        y=\"0\",\n",
    "        x=\"1\",\n",
    "        hover_cols=[\"2\"],\n",
    "        xlabel=\"False Positive Rate = Pr(S=1 | G=g, T=0)\",\n",
    "        ylabel=\"True Positive Rate = Pr(S=1 | G=g, T=1)\",\n",
    "    )\n",
    "    ROC_plot = pd.DataFrame(ROC_g).hvplot(**kwargs).relabel(\"G=1\") * pd.DataFrame(\n",
    "        ROC_G\n",
    "    ).hvplot(**kwargs).relabel(\"G=2\")\n",
    "    mids = pd.DataFrame(g_mid).hvplot.scatter(**kwargs) * pd.DataFrame(\n",
    "        G_mid\n",
    "    ).hvplot.scatter(**kwargs)\n",
    "    return ROC_plot * mids\n",
    "\n",
    "\n",
    "plots = ROC_curve(clfs[0]).opts(title=str(clfs[0])) + ROC_curve(clfs[1]).opts(\n",
    "    title=str(clfs[1])\n",
    ")\n",
    "plots.cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating statistical parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_parity(pred: np.array, lowest_pred) -> np.array:\n",
    "    ratio_delta = pred.mean() - lowest_pred\n",
    "    n_remove = int(len(pred) * ratio_delta)\n",
    "    remove_idx = np.random.choice(np.where(pred == 1)[0], size=n_remove, replace=False)\n",
    "    pred[remove_idx] = 0\n",
    "    return pred\n",
    "\n",
    "\n",
    "predictions = []\n",
    "scores = []\n",
    "for clf in clfs:\n",
    "    preds = [clf.predict(X_test[mask]) for mask in test_group_masks]\n",
    "    lowest_pred = min(pred.mean() for pred in preds)\n",
    "    preds = [statistical_parity(pred, lowest_pred) for pred in preds]\n",
    "    predictions.append(preds)\n",
    "    scores.append([pred.mean() for pred in preds])\n",
    "\n",
    "new_metrics = (\n",
    "    pd.DataFrame(scores, columns=[\"SP_1\", \"SP_2\"], index=[\"WhiteBox\", \"BlackBox\"])\n",
    "    .rename_axis(\"Parity positive [%]\")\n",
    "    .round(2)\n",
    ")\n",
    "new_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_group_df(group, group_no, clf):\n",
    "    S = pd.Series(predictions[clf][group_no], name=\"S\")\n",
    "    T = pd.Series(y_test[group], name=\"T\")\n",
    "    G = pd.Series([bool(group_no)] * len(T), name=\"G\")\n",
    "    return pd.concat([S, G, T], axis=1)\n",
    "\n",
    "\n",
    "for i, box in enumerate(df_boxes):\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            get_new_group_df(group, group_no, i)\n",
    "            for group_no, group in enumerate(test_group_masks)\n",
    "        ],\n",
    "    )\n",
    "    # G is group\n",
    "    # T is target\n",
    "    # S is prediction/selection\n",
    "    N = lambda string: len(df.query(string))\n",
    "    new_metrics.loc[box, \"E_odd_t0_g0\"] = N(\"~T and S and ~G\") / N(\"~G and ~T\")\n",
    "    new_metrics.loc[box, \"E_odd_t0_g1\"] = N(\"~T and S and G\") / N(\"G and ~T\")\n",
    "    new_metrics.loc[box, \"E_odd_t1_g0\"] = N(\"T and S and ~G\") / N(\"~G and T\")\n",
    "    new_metrics.loc[box, \"E_odd_t1_g1\"] = N(\"T and S and G\") / N(\"G and T\")\n",
    "    # Equalized outcome 0 is precision\n",
    "    new_metrics.loc[box, \"E_out_s0_g0\"] = N(\"~G and T and ~S\") / N(\"~G and ~S\")\n",
    "    new_metrics.loc[box, \"E_out_s0_g1\"] = N(\"G and T and ~S\") / N(\"G and ~S\")\n",
    "    new_metrics.loc[box, \"E_out_s1_g0\"] = N(\"~G and T and S\") / N(\"~G and S\")\n",
    "    new_metrics.loc[box, \"E_out_s1_g1\"] = N(\"G and T and S\") / N(\"G and S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = (\n",
    "    parity_df.stack()\n",
    "    .rename_axis([\"box\", \"category\"])\n",
    "    .rename(\"prob\")\n",
    "    .reset_index()\n",
    "    .assign(color=[\"#4477AA\", \"#EE6677\"] * 2)\n",
    "    .replace({\"Group 1\": \"Pr(Selection | Male)\", \"Group 2\": \"Pr(Selection | Female)\"})\n",
    ")\n",
    "plots = plot_df.hvplot.bar(\n",
    "    x=\"category\", width=600, groupby=\"box\", color=\"color\", invert=True, ylim=(0, 1)\n",
    ")\n",
    "plot_df\n",
    "\n",
    "parity_plots = (\n",
    "    plots[\"WhiteBox\"].opts(title=\"Statistical Parity (Whitebox)\")\n",
    "    + plots[\"BlackBox\"].opts(\n",
    "        width=400, yaxis=None, title=\"Statistical Parity (Blackbox)\"\n",
    "    )\n",
    ").opts(shared_axes=False)\n",
    "\n",
    "plots = (\n",
    "    (\n",
    "        parity_plots[0].opts(width=450)\n",
    "        + parity_plots[1].opts(width=250)\n",
    "        + get_eq_odds_plot(\"WhiteBox\", df_metrics=new_metrics).opts(width=450)\n",
    "        + get_eq_odds_plot(\"BlackBox\", df_metrics=new_metrics).opts(\n",
    "            yaxis=None, width=250\n",
    "        )\n",
    "        + get_eq_out_plot(\"WhiteBox\", df_metrics=new_metrics).opts(width=450)\n",
    "        + get_eq_out_plot(\"BlackBox\", df_metrics=new_metrics).opts(\n",
    "            yaxis=None, width=250\n",
    "        )\n",
    "    )\n",
    "    .cols(2)\n",
    "    .opts(shared_axes=False)\n",
    "    .opts(hv.opts.Bars(ylabel=\"\", xlabel=\"\", xaxis=None, height=150))\n",
    ")\n",
    "plots[-1].opts(xaxis=True)\n",
    "plots[-2].opts(xaxis=True)\n",
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metrics = new_metrics.rename_axis(\"\")\n",
    "new_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference for each metric before doing statistical parity\n",
    "between_group_difference_before = pd.DataFrame()\n",
    "between_group_difference_before[\"E_odd_false_positive\"] = (\n",
    "    df_metrics[\"E_odd_t0_g0\"] - df_metrics[\"E_odd_t0_g1\"]\n",
    ")\n",
    "between_group_difference_before[\"E_odd_true_positive\"] = (\n",
    "    df_metrics[\"E_odd_t1_g0\"] - df_metrics[\"E_odd_t1_g1\"]\n",
    ")\n",
    "between_group_difference_before[\"E_out_false_positive\"] = (\n",
    "    df_metrics[\"E_out_s0_g0\"] - df_metrics[\"E_out_s0_g1\"]\n",
    ")\n",
    "between_group_difference_before[\"E_out_true_positive\"] = (\n",
    "    df_metrics[\"E_out_s1_g0\"] - df_metrics[\"E_out_s1_g1\"]\n",
    ")\n",
    "between_group_difference_before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference for each metric after doing statistical parity\n",
    "between_group_difference_after = pd.DataFrame()\n",
    "between_group_difference_after[\"E_odd_false_positive\"] = (\n",
    "    new_metrics[\"E_odd_t0_g0\"] - new_metrics[\"E_odd_t0_g1\"]\n",
    ")\n",
    "between_group_difference_after[\"E_odd_true_positive\"] = (\n",
    "    new_metrics[\"E_odd_t1_g0\"] - new_metrics[\"E_odd_t1_g1\"]\n",
    ")\n",
    "between_group_difference_after[\"E_out_false_positive\"] = (\n",
    "    new_metrics[\"E_out_s0_g0\"] - new_metrics[\"E_out_s0_g1\"]\n",
    ")\n",
    "between_group_difference_after[\"E_out_true_positive\"] = (\n",
    "    new_metrics[\"E_out_s1_g0\"] - new_metrics[\"E_out_s1_g1\"]\n",
    ")\n",
    "between_group_difference_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_inequality = abs(between_group_difference_before) \n",
    "after_statistical_parity_inequality = abs(between_group_difference_after)\n",
    "decrease_in_inequality = baseline_inequality - after_statistical_parity_inequality\n",
    "display(decrease_in_inequality.round(2))\n",
    "# Positive value means closer to equality.\n",
    "# We can see the white box model get closer to equalized odds but much further away from equalized outcomes\n",
    "# The black box model doesn't change as much since it was much closer to statistical parity already. It gets further away from equalized odds but closer to equalized outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "1. Explain the trained model of your white-box approach (logistic regression\n",
    "or the decision tree). In particular, discuss which features in the model\n",
    "are deemed most relevant. Reflect on the interpretation. Does it fit your\n",
    "intuition about the prediction task?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = make_pipeline(MinMaxScaler(), LogisticRegression(random_state=0))\n",
    "coefs = []\n",
    "coef = lr_clf.fit(X_train, y_train)[1].coef_\n",
    "number_of_samples = 100\n",
    "for i in trange(number_of_samples):\n",
    "    X_train_sample = X_train.assign(y_train=y_train).sample(frac=1.0, replace=True)\n",
    "    y_train_sample = X_train_sample[\"y_train\"]\n",
    "    X_train_sample = X_train[X_train.columns]\n",
    "    coefs.append(lr_clf.fit(X_train_sample, y_train_sample)[1].coef_)\n",
    "coefs_ = [list(coef[0]) for coef in coefs]\n",
    "coef_df = pd.DataFrame(coefs_, columns=X_train.columns)\n",
    "coef_stds = coef_df.std()\n",
    "coef_stderr = coef_stds / number_of_samples\n",
    "coef_stderr\n",
    "\n",
    "ab_coef = np.abs(coef).reshape(\n",
    "    54,\n",
    ")\n",
    "feature_importance = ab_coef / coef_stderr\n",
    "np.argsort(feature_importance)\n",
    "feature_importance_sorted = feature_importance.sort_values(ascending=False)\n",
    "feature_importance_sorted.head(5).sort_values().hvplot.bar(\n",
    "    invert=True, width=300, height=150, title=\"Top 5 Feature Importance\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 part 1. Feature importance shows that sex , Age and university schooling have large predictive power for income. This makes sense as the older a person is the further on in their career they will be. Gender pay discrimination in favor of men is also a thing that would therefore improve the predictive power of the model. University level schooling gives access to higher paying jobs and thus it makes sense that this feature has high predictive power for income. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.copy()\n",
    "sex_col = X_train.columns.to_list().index('SEX_2')\n",
    "baseline_sex = X_test.iloc[0, sex_col]\n",
    "baseline_pred = clf1.predict(X.iloc[0:1])[0]\n",
    "X.iloc[0, sex_col] = ~baseline_sex\n",
    "counterfactual_pred = clf1.predict(X.iloc[0:1])[0]\n",
    "print(f'{baseline_pred = }\\n{counterfactual_pred = }')\n",
    "assert baseline_pred != counterfactual_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex_2 is whether the person is female i.e 1 they are female.\n",
    "This example shows that when you change the sex, the classification can change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "explainers = [\n",
    "    shap.Explainer,\n",
    "    shap.TreeExplainer,\n",
    "]\n",
    "for clf, explainer in zip(clfs, explainers):\n",
    "    n_samples_shown = 1000\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Shap doesn't work with pipeline classifiers\n",
    "    transformer, model = [obj for name, obj in clf.steps]\n",
    "    data = pd.DataFrame(\n",
    "        transformer.transform(X_train), columns=X_train.columns, index=X_train.index\n",
    "    )\n",
    "    sample = data.sample(n_samples_shown)\n",
    "\n",
    "    shap_values = explainer(model, data).shap_values(sample)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(str(model))\n",
    "    shap.summary_plot(shap_values, sample, feature_names=X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "5000f566ae852e83166ca6f17c08f4027204c584903a450ab8f91dfaa0546e2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
